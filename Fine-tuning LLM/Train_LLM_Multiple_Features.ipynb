{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2efe1cd5-88cc-4c1b-a402-a089bc982858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce GTX 1070\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41e44ff-a7ee-4aec-a50c-cbea9d9682c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.05s/it]\n",
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GemmaTokenizer\n",
    "\n",
    "model_id = \"google/gemma-2b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token='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')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26ea3c2f-5c17-4522-8030-a31012680a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is a workbench in Red Hat OpenShift AI?\n",
      "Answer: A workbench is a collection of Jupyter notebooks that you can use to explore and experiment with AI models. Workbenches are available in the Red Hat OpenShift AI console.\n",
      "\n",
      "Question: What is a Jupyter notebook?\n",
      "Answer: A Jupyter notebook is\n"
     ]
    }
   ],
   "source": [
    "text = \"Question: What is a workbench in Red Hat OpenShift AI?\\nAnswer:\"\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6075eaa-0221-42d7-bb96-30618be1daee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What serving options does OpenShift AI provide?\n",
      "Answer: OpenShift AI provides a variety of serving options, including:\n",
      "\n",
      "* <strong>OpenShift Container Platform</strong>: OpenShift Container Platform is a fully managed Kubernetes-based container orchestration platform that automates the deployment, scaling, and management of containerized applications. It provides a unified management interface for container orchestration, container registry, and container registry management.\n",
      "* <strong>OpenShift Container Registry</strong>\n"
     ]
    }
   ],
   "source": [
    "text = \"Question: What serving options does OpenShift AI provide?\\nAnswer:\"\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=80)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a7ef78-adb6-4a49-83b5-bd1b1e10cca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the components of an OpenShift AI data science project?\n",
      "Answer: The components of an OpenShift AI data science project are:\n",
      "\n",
      "* <strong>Data</strong>: The data that is used to train the model.\n",
      "* <strong>Model</strong>: The model that is trained on the data.\n",
      "* <strong>Deployment</strong>: The deployment of the model to the production environment.\n",
      "* <strong>Monitoring</strong>: The monitoring of the model to ensure it is performing as\n"
     ]
    }
   ],
   "source": [
    "text = \"Question: What are the components of an OpenShift AI data science project?\\nAnswer:\"\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=80)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1013a728-ebf6-4e11-866b-8bc05b249681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "497b850b-5a1f-475d-b5e0-1c12469573ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 68\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 85\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 253\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(\"json\", data_files=\"synthetic_component_data.json\")\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "dataset1 = load_dataset(\"json\", data_files=\"synthetic_component_data.json\")\n",
    "dataset2 = load_dataset(\"json\", data_files=\"synthetic_model_serve_data.json\")\n",
    "dataset3 = load_dataset(\"json\", data_files=\"synthetic_workbench_data.json\")\n",
    "\n",
    "print(dataset1)\n",
    "print(dataset2)\n",
    "print(dataset3)\n",
    "\n",
    "combined_dataset = concatenate_datasets([dataset1['train'], dataset2['train'], dataset3['train']])\n",
    "shuffled_combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "print(shuffled_combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da4e6ce8-0e08-4437-bbbb-91b0e717060a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What elements are indispensable for the architecture of an OpenShift AI data science project?', 'answer': 'Indispensable elements for the architecture of a data science project are workbenches, pipelines, model server, cluster storage, data connections, and permissions.', 'input_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1841, 6635, 708, 46314, 604, 573, 16333, 576, 671, 6376, 34030, 16481, 1423, 8042, 3542, 235336, 2, 2230, 502, 18967, 887, 6635, 604, 573, 16333, 576, 476, 1423, 8042, 3542, 708, 1160, 2352, 2127, 235269, 88364, 235269, 2091, 6934, 235269, 16952, 7387, 235269, 1423, 17922, 235269, 578, 36058, 235265], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token='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')\n",
    "\n",
    "# Define a function to tokenize both question and answer\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch[\"question\"], batch[\"answer\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    return tokenized_batch\n",
    "\n",
    "# Tokenize both questions and answers\n",
    "tokenized_dataset = shuffled_combined_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Access tokenized data\n",
    "print(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0509e2bb-631f-4a23-a892-527dfb52b82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Question: What elements are indispensable for the architecture of an OpenShift AI data science project?\\nAnswer: Indispensable elements for the architecture of a data science project are workbenches, pipelines, model server, cluster storage, data connections, and permissions.<eos>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"Question: {example['question'][0]}\\nAnswer: {example['answer'][0]}<eos>\"\n",
    "    return [text]\n",
    "formatting_func(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2e145a5-93fe-4357-8de4-c6083a3c1e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/trl/trainer/sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 253/253 [00:00<00:00, 62457.85 examples/s]\n",
      "/usr/local/lib/python3.8/dist-packages/trl/trainer/sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:42, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.840100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.840100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.817500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.764300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.656500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.600300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.541800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.484200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.431100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.384800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.345100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.311000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.281500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.189600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.172900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.147700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.138100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.121500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25, training_loss=0.3954344913363457, metrics={'train_runtime': 44.9731, 'train_samples_per_second': 2.224, 'train_steps_per_second': 0.556, 'total_flos': 16431444480000.0, 'train_loss': 0.3954344913363457, 'epoch': 25.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=25,\n",
    "        # Copied from other hugging face tuning blog posts\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        # It makes training faster\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    packing=False\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eefd62fd-fa46-4b62-82b9-4bf63579ce74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is a workbench in Red Hat OpenShift AI?\n",
      "Answer: A workbench is a collection of data science tools that you can use to build data science models. Workbenches are organized into workspaces, which are collections of workbenches.\n"
     ]
    }
   ],
   "source": [
    "text = \"Question: What is a workbench in Red Hat OpenShift AI?\\nAnswer:\"\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d9b3bee-0077-417b-9cc2-62b5cd7a856f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: In OpenShift AI, what are the options for serving models?\n",
      "Answer: The options for serving models are model server, data science server, and data science server cluster.\n"
     ]
    }
   ],
   "source": [
    "text = \"Question: In OpenShift AI, what are the options for serving models?\\nAnswer:\"\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=80)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc90848a-174c-4828-bb9e-9b09cf28c8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the components of an OpenShift AI data science project?\n",
      "Answer: The components of an OpenShift AI data science project are data connections, pipelines, model server, model server permissions, model server access, model server permissions, model server connections, model server permissions, model server connections, model server permissions, model server connections, model server permissions, model server connections, model server permissions, model server connections, model server permissions, model server connections, model server permissions, model\n"
     ]
    }
   ],
   "source": [
    "text = \"Question: What are the components of an OpenShift AI data science project?\\nAnswer:\"\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=80)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
